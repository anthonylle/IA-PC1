{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descripción\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se basa en la técnica Gradient Descent. Sirve para buscar mínimos locales en una función objetivo para entrenar modelos. Dichos modelos pueden tener muchos datos de entrenamiento: miles, incluso millones; y esto puede causar un gran costo en el cálculo del gradiente de la función. La variante Stochastic se basa en la aleatoriedad para seleccionar minibatchs de un tamano fijo m, que es un subconjunto de los datos de entrenamiento para aproximar el gradiente. Un ejemplo de esto es la función:  \\begin{equation*} \\dot{\\Theta} = \\underset{\\Theta}{\\operatorname{argmax}} \\sum_{i = 1}^{n} \\log{p(B_i, A_i)}.\n",
    " \\end{equation*}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
